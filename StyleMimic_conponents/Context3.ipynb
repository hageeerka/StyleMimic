{"cells":[{"cell_type":"markdown","source":["**ЗАПУСТИ МЕНЯ!** Загрузка стоп-слов, пункутации и т.д. Также открываем доступ к к папке в гугл диске где находится переписки"],"metadata":{"id":"EiuL3iKPlWd4"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')\n","\n","from google.colab import drive\n","drive.mount('/content/drive') #Открываем папку"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wwWNlMSFFrUP","executionInfo":{"status":"ok","timestamp":1734539990648,"user_tz":-300,"elapsed":20876,"user":{"displayName":"george lapp","userId":"04847527313554007452"}},"outputId":"09f750ce-6388-4bf3-a788-ec0c655c12c8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["Алгоритм для проверки частоты и уникальности слова. Этот код как **примерная** демонстрация работы кода в общем."],"metadata":{"id":"O3jjcHVQx_hr"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":425,"status":"ok","timestamp":1734540004780,"user":{"displayName":"george lapp","userId":"04847527313554007452"},"user_tz":-300},"id":"_Rvnpxhm6XDQ","outputId":"289c58b3-343e-4216-eeb3-22420da477f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ключевые слова Андрея: [('ёмае', 0.2755907325005277), ('классно', 0.1058484534202508), ('ёмае классно', 0.1058484534202508), ('актовом', 0.09622504486493762), ('кодь', 0.09622504486493762), ('кодь актовом', 0.09622504486493762), ('лево', 0.09622504486493762), ('право', 0.09622504486493762), ('право лево', 0.09622504486493762), ('закончим', 0.05148523336763505)]\n","Ключевые слова Николая: [('футболиста', 0.16666666666666666), ('аттестата', 0.07453559924999298), ('аттестата спрошу', 0.07453559924999298), ('втором', 0.07453559924999298), ('втором ряде', 0.07453559924999298), ('комиссии', 0.07453559924999298), ('конца', 0.07453559924999298), ('насчёт', 0.07453559924999298), ('насчёт аттестата', 0.07453559924999298), ('подойду', 0.07453559924999298)]\n"]}],"source":["import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","# Скачиваем необходимые ресурсы\n","\n","\n","# Предобработка текста\n","def preprocess_text(text):\n","    \"\"\"\n","    Очищает текст: убирает пунктуацию, приводит к нижнему регистру, удаляет стоп-слова.\n","    \"\"\"\n","    # Убираем пунктуацию и приводим к нижнему регистру\n","    text = re.sub(r'[^\\w\\s]', '', text.lower())\n","    # Токенизация\n","    words = word_tokenize(text)\n","    # Удаляем стоп-слова\n","    stop_words = set(stopwords.words('russian'))\n","    return ' '.join([word for word in words if word not in stop_words and len(word) > 2])\n","\n","# Анализ стиля\n","def extract_style_keywords(messages, top_n=10):\n","    \"\"\"\n","    Выделяет ключевые слова, характерные для стиля конкретного человека.\n","\n","    Args:\n","        messages: Список сообщений человека.\n","        top_n: Количество ключевых слов для извлечения.\n","    Returns:\n","        Список ключевых слов с их важностью.\n","    \"\"\"\n","    # Предобрабатываем все сообщения\n","    preprocessed_messages = [preprocess_text(msg) for msg in messages]\n","\n","    # Используем TF-IDF для анализа частотности и уникальности\n","    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=1000)  # Одно- и двусловные фразы\n","    tfidf_matrix = vectorizer.fit_transform(preprocessed_messages)\n","\n","    # Получаем слова и их веса\n","    feature_names = vectorizer.get_feature_names_out()\n","    tfidf_scores = tfidf_matrix.mean(axis=0).A1  # Средний вес для каждого слова\n","    keywords = list(zip(feature_names, tfidf_scores))\n","\n","    # Сортируем по важности\n","    keywords = sorted(keywords, key=lambda x: x[1], reverse=True)\n","\n","    return keywords[:top_n]\n","\n","\n","# Пример данных\n","example_chat = {\n","    \"Андрей\": [\n","        \"Коль я своиз нашел ёмае, потому как закончим напишу\",\n","        \"Кодь я уже в актовом, ты где был\",\n","        \"А право или лево\",\n","        \"Ёмае\",\n","        \"Привет, прости,Ёмае, что долго я думаю примерно 10-20\",\n","        \"Ёмае, тут так классно!\",\n","    ],\n","    \"Николай\": [\n","        \"Хорошо\",\n","        \"Класссс, Андрей! подрапали отсюда\",\n","        \"Я на втором ряде с конца\",\n","        \"Я сейчас подойду к приёмной комиссии\",\n","        \"Насчёт аттестата спрошу\",\n","        \"Я уже у футболиста\"\n","    ]\n","}\n","\n","# Анализируем стиль Андрея\n","andrey_keywords = extract_style_keywords(example_chat[\"Андрей\"], top_n=10)\n","print(\"Ключевые слова Андрея:\", andrey_keywords)\n","\n","# Анализируем стиль Николая\n","nikolay_keywords = extract_style_keywords(example_chat[\"Николай\"], top_n=10)\n","print(\"Ключевые слова Николая:\", nikolay_keywords)\n"]},{"cell_type":"markdown","source":["Вариант-1"],"metadata":{"id":"YGVUGw4gyLDU"}},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"elapsed":338,"status":"error","timestamp":1734540015645,"user":{"displayName":"george lapp","userId":"04847527313554007452"},"user_tz":-300},"id":"vwrSgbN0dWya","outputId":"592af40b-80fa-4d99-d7dd-d72ddbc01863"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/StyleMimic/messages.json'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-32a452a99d81>\u001b[0m in \u001b[0;36m<cell line: 109>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m# Получаем ключевые слова\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mstyle_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperson_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Ключевые слова для {person_name}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstyle_keywords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-32a452a99d81>\u001b[0m in \u001b[0;36manalyze_style\u001b[0;34m(file_path, person_name, top_n)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# Загружаем данные из JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/StyleMimic/messages.json'"]}],"source":["import json\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk import download\n","\n","# Скачиваем необходимые ресурсы для обработки текста\n","\n","\n","# Получаем список русских стоп-слов и преобразуем в список\n","russian_stopwords = list(stopwords.words('russian'))\n","\n","# Функция для предобработки текста\n","def preprocess_text(text):\n","    \"\"\"\n","    Очищает текст: убирает пунктуацию, приводит к нижнему регистру, удаляет стоп-слова.\n","    \"\"\"\n","    # Убираем пунктуацию и приводим к нижнему регистру\n","    text = re.sub(r'[^\\w\\s]', '', text.lower())\n","    # Токенизация\n","    words = word_tokenize(text)\n","    # Удаляем стоп-слова\n","    custom_stop_words = [\n","        'это', 'всё', 'тебе', 'просто', 'очень', 'ладно', 'ещё',\n","        'давай', 'ага', 'так', 'как', 'мне', 'ты', 'он', 'она',\n","        'мы', 'вы', 'кто', 'где', 'что'\n","        ]\n","      # это список стоп слов который мы можем пожеланию дополнять стоп словами, по нашему мнению, точно ничего не характеризующие. Иначе используем стандартный russian_stopwords\n","\n","    return ' '.join([word for word in words if word not in russian_stopwords and len(word) > 2])\n","\n","# Функция для выделения ключевых слов\n","def extract_style_keywords(messages, top_n=10):\n","    \"\"\"\n","    Выделяет ключевые слова, характерные для стиля конкретного человека.\n","\n","    Args:\n","        messages: Список сообщений человека.\n","        top_n: Количество ключевых слов для извлечения.\n","    Returns:\n","        Список ключевых слов с их важностью.\n","    \"\"\"\n","    # Предобрабатываем все сообщения\n","    preprocessed_messages = [preprocess_text(msg) for msg in messages]\n","\n","    # Используем TF-IDF для анализа частотности и уникальности\n","    vectorizer = TfidfVectorizer(\n","        ngram_range=(1, 3),    # Выделяются фразы от N до M слов (например все фразы имеющие от 1 до 3 слов)\n","        max_features=50,      # Максимум N фраз\n","        #ЧЕМ БОЛЬШЕ НЕЗНАЧИТЕЛЬНЫХ ФРАЗ, ТЕМ МЕНЬШЕ ВЫСТАВЛЯЕМ КОЛИЧЕСТВО ВЫДЕЛЯЕМЫХ КЛЮЧ-ФРАЗ\n","        min_df=100,               # Минимум N данных слов из документа.Чем больше значение, тем меньше выводит редких слов\n","        #Увеличьте значение, если хотите игнорировать более редкие слова (например, 5 или 10).\n","        #Уменьшите, если вам нужно включить больше уникальных, но редких слов (например, 1 или 2).\n","        max_df=0.008,            # Максимум N% документов (от 0 до 100%, включая доли процентов- 0.00001%)\n","        # Увеличьте значение (например, 0.9), чтобы включить больше слов, если они все еще являются значимыми для анализа.\n","        # Уменьшите (например, 0.008), если хотите исключить слишком распространенные слова (например, \"это\", \"завтра\").\n","        # ЗНАЧЕНИЕ ВЫШЕ НЕОБХОДИМО МЕНЯТЬ В ЗАВИСИМОСТИ ОТ ВЫВОДА.\n","        stop_words=russian_stopwords  # Убираем стоп-слова (либо russian_stopwords либо custom_stop_words)\n","    )\n","    tfidf_matrix = vectorizer.fit_transform(preprocessed_messages)\n","\n","    # Получаем слова и их веса\n","    feature_names = vectorizer.get_feature_names_out()\n","    tfidf_scores = tfidf_matrix.mean(axis=0).A1  # Средний вес для каждого слова\n","    keywords = list(zip(feature_names, tfidf_scores))\n","\n","    # Сортируем по важности\n","    keywords = sorted(keywords, key=lambda x: x[1], reverse=True)\n","\n","    return keywords[:top_n]\n","\n","# Основной код\n","def analyze_style(file_path, person_name, top_n=10):\n","    \"\"\"\n","    Анализирует стиль конкретного человека на основе JSON-файла с сообщениями.\n","\n","    Args:\n","        file_path: Путь к JSON-файлу.\n","        person_name: Имя человека, чьи сообщения нужно анализировать.\n","        top_n: Количество ключевых слов для извлечения.\n","    Returns:\n","        Список ключевых слов с их важностью.\n","    \"\"\"\n","    # Загружаем данные из JSON\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        data = json.load(file)\n","\n","    # Извлекаем сообщения конкретного человека\n","    messages = []\n","    for person in data.keys():\n","        for entry in data.get(person, []):\n","            if entry['from'] == person_name:\n","                messages.append(entry['text'])\n","\n","    if not messages:\n","        print(f\"Нет сообщений для пользователя: {person_name}\")\n","        return []\n","\n","    # Анализируем стиль и возвращаем ключевые слова\n","    keywords = extract_style_keywords(messages, top_n=top_n)\n","    return keywords\n","\n","# Пример использования\n","file_path = '/content/drive/MyDrive/StyleMimic/messages.json'  # Укажите путь к JSON-файлу\n","person_name = 'Никитенко Николай'  # Имя человека, чьи сообщения нужно анализировать\n","\n","# Получаем ключевые слова\n","style_keywords = analyze_style(file_path, person_name, top_n=10**9)\n","print(f\"Ключевые слова для {person_name}:\")\n","for word, score in style_keywords:\n","    print(f\"{word}: {score.round(4)}\")\n"]},{"cell_type":"markdown","source":["Вариант-2"],"metadata":{"id":"Gbd6L7QzyOQn"}},{"cell_type":"code","source":["import json\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk import download\n","\n","# Скачиваем необходимые ресурсы для обработки текста\n","download('punkt')\n","download('stopwords')\n","\n","# Получаем список русских стоп-слов и преобразуем в список\n","russian_stopwords = list(stopwords.words('russian'))\n","\n","# Функция для предобработки текста\n","def preprocess_text(text):\n","    \"\"\"\n","    Очищает текст: убирает пунктуацию, приводит к нижнему регистру, удаляет стоп-слова.\n","    \"\"\"\n","    # Убираем пунктуацию и приводим к нижнему регистру\n","    text = re.sub(r'[^\\w\\s]', '', text.lower())\n","    # Токенизация\n","    words = word_tokenize(text)\n","    # Удаляем стоп-слова\n","    custom_stop_words = [\n","        'это', 'всё', 'тебе', 'просто', 'очень', 'ладно', 'ещё',\n","        'давай', 'ага', 'так', 'как', 'мне', 'ты', 'он', 'она',\n","        'мы', 'вы', 'кто', 'где', 'что'\n","        ]\n","      # это список стоп слов который мы можем пожеланию дополнять стоп словами, по нашему мнению, точно ничего не характеризующие. Иначе используем стандартный russian_stopwords\n","\n","    return ' '.join([word for word in words if word not in russian_stopwords and len(word) > 2])\n","\n","\n","# Функция по удалению дубликатов ключевых слов\n","def remove_dublicates(keywords):\n","\n","  sorted_keywords = sorted(keywords, key=lambda x: (-len(x[0].split()), x[1]), reverse=True)# Сортировака по длине фразы\n","  filtered_keywords = []\n","  seen_phrases = set()\n","\n","  for phrase, score in sorted_keywords:\n","      if not any(phrase in longer for longer in seen_phrases): #Проверка на присутствие дубликата\n","          filtered_keywords.append((phrase, score))\n","          seen_phrases.add(phrase)\n","      # else:\n","      #     print(phrase)\n","  return filtered_keywords\n","\n","\n","# Функция для выделения ключевых слов\n","def extract_style_keywords(messages, top_n=10):\n","    \"\"\"\n","    Выделяет ключевые слова, характерные для стиля конкретного человека.\n","\n","    Args:\n","        messages: Список сообщений человека.\n","        top_n: Количество ключевых слов для извлечения.\n","    Returns:\n","        Список ключевых слов с их важностью.\n","    \"\"\"\n","    # Предобрабатываем все сообщения\n","    preprocessed_messages = [preprocess_text(msg) for msg in messages]\n","\n","    # Используем TF-IDF для анализа частотности и уникальности\n","    vectorizer = TfidfVectorizer(\n","        ngram_range=(1, 3),    # Выделяются фразы от N до M слов (например все фразы имеющие от 1 до 3 слов)\n","        max_features=50,      # Максимум N фраз\n","        #ЧЕМ БОЛЬШЕ НЕЗНАЧИТЕЛЬНЫХ ФРАЗ, ТЕМ МЕНЬШЕ ВЫСТАВЛЯЕМ КОЛИЧЕСТВО ВЫДЕЛЯЕМЫХ КЛЮЧ-ФРАЗ\n","        min_df=100,               # Минимум N данных слов из документа.Чем больше значение, тем меньше выводит редких слов\n","        #Увеличьте значение, если хотите игнорировать более редкие слова (например, 5 или 10).\n","        #Уменьшите, если вам нужно включить больше уникальных, но редких слов (например, 1 или 2).\n","        max_df=0.008,            # Максимум N% документов (от 0 до 100%, включая доли процентов- 0.00001%)\n","        # Увеличьте значение (например, 0.9), чтобы включить больше слов, если они все еще являются значимыми для анализа.\n","        # Уменьшите (например, 0.008), если хотите исключить слишком распространенные слова (например, \"это\", \"завтра\").\n","        # ЗНАЧЕНИЕ ВЫШЕ НЕОБХОДИМО МЕНЯТЬ В ЗАВИСИМОСТИ ОТ ВЫВОДА.\n","        stop_words=russian_stopwords  # Убираем стоп-слова (либо russian_stopwords либо custom_stop_words)\n","    )\n","    tfidf_matrix = vectorizer.fit_transform(preprocessed_messages)\n","\n","    # Получаем слова и их веса\n","    feature_names = vectorizer.get_feature_names_out()\n","    tfidf_scores = tfidf_matrix.mean(axis=0).A1  # Средний вес для каждого слова\n","    keywords = list(zip(feature_names, tfidf_scores))\n","\n","    # Сортируем по важности\n","    keywords = sorted(keywords, key=lambda x: x[1], reverse=True)\n","\n","    keywords = remove_dublicates(keywords)\n","\n","    return keywords[:top_n]\n","\n","# Основной код\n","def analyze_style(file_path, person_name, top_n=10):\n","    \"\"\"\n","    Анализирует стиль конкретного человека на основе JSON-файла с сообщениями.\n","\n","    Args:\n","        file_path: Путь к JSON-файлу.\n","        person_name: Имя человека, чьи сообщения нужно анализировать.\n","        top_n: Количество ключевых слов для извлечения.\n","    Returns:\n","        Список ключевых слов с их важностью.\n","    \"\"\"\n","    # Загружаем данные из JSON\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        data = json.load(file)\n","\n","    # Извлекаем сообщения конкретного человека\n","    messages = []\n","    for person in data.keys():\n","        for entry in data.get(person, []):\n","            if entry['from'] == person_name:\n","                messages.append(entry['text'])\n","\n","    if not messages:\n","        print(f\"Нет сообщений для пользователя: {person_name}\")\n","        return []\n","\n","    # Анализируем стиль и возвращаем ключевые слова\n","    keywords = extract_style_keywords(messages, top_n=top_n)\n","    return keywords\n","\n","# Пример использования\n","file_path = '/content/drive/MyDrive/StyleMimic/messages.json'  # Укажите путь к JSON-файлу\n","person_name = 'Никитенко Николай'  # Имя человека, чьи сообщения нужно анализировать\n","\n","# Получаем ключевые слова\n","style_keywords = analyze_style(file_path, person_name, top_n=10)\n","print(len(style_keywords))\n","print(f\"Ключевые слова для {person_name}:\")\n","for word, score in style_keywords:\n","    print(f\"{word}: {score.round(4)}\")\n"],"metadata":{"id":"WGlzMzz3P9fI","executionInfo":{"status":"aborted","timestamp":1734539958398,"user_tz":-300,"elapsed":4,"user":{"displayName":"george lapp","userId":"04847527313554007452"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Вариант-3"],"metadata":{"id":"VmxLXDwFnQ1A"}},{"cell_type":"code","source":["import json\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk import download\n","\n","# Скачиваем необходимые ресурсы для обработки текста\n","download('punkt')\n","download('stopwords')\n","\n","# Получаем список русских стоп-слов и преобразуем в список\n","russian_stopwords = list(stopwords.words('russian'))\n","\n","# Функция для предобработки текста\n","def preprocess_text(text):\n","    \"\"\"\n","    Очищает текст: убирает пунктуацию, приводит к нижнему регистру, удаляет стоп-слова.\n","    \"\"\"\n","    # Убираем пунктуацию и приводим к нижнему регистру\n","    text = re.sub(r'[^\\w\\s]', '', text.lower())\n","    # Токенизация\n","    words = word_tokenize(text)\n","    # Удаляем стоп-слова\n","    custom_stop_words = [\n","        'это', 'всё', 'тебе', 'просто', 'очень', 'ладно', 'ещё',\n","        'давай', 'ага', 'так', 'как', 'мне', 'ты', 'он', 'она',\n","        'мы', 'вы', 'кто', 'где', 'что'\n","        ]\n","      # это список стоп слов который мы можем пожеланию дополнять стоп словами, по нашему мнению, точно ничего не характеризующие. Иначе используем стандартный russian_stopwords\n","\n","    return ' '.join([word for word in words if word not in russian_stopwords and len(word) >= 3])\n","\n","# Функция для выделения ключевых слов\n","def extract_style_keywords(messages, top_n=10):\n","    \"\"\"\n","    Выделяет ключевые слова, характерные для стиля конкретного человека.\n","\n","    Args:\n","        messages: Список сообщений человека.\n","        top_n: Количество ключевых слов для извлечения.\n","    Returns:\n","        Список ключевых слов с их важностью.\n","    \"\"\"\n","    # Предобрабатываем все сообщения\n","    preprocessed_messages = [preprocess_text(msg) for msg in messages]\n","\n","    # Используем TF-IDF для анализа частотности и уникальности\n","    vectorizer = TfidfVectorizer(\n","        ngram_range=(1, 3),    # Выделяются фразы от N до M слов (например все фразы имеющие от 1 до 3 слов)\n","        max_features=50,      # Максимум N фраз\n","        #ЧЕМ БОЛЬШЕ НЕЗНАЧИТЕЛЬНЫХ ФРАЗ, ТЕМ МЕНЬШЕ ВЫСТАВЛЯЕМ КОЛИЧЕСТВО ВЫДЕЛЯЕМЫХ КЛЮЧ-ФРАЗ\n","        min_df=100,               # Минимум N данных слов из документа.Чем больше значение, тем меньше выводит редких слов\n","        #Увеличьте значение, если хотите игнорировать более редкие слова (например, 5 или 10).\n","        #Уменьшите, если вам нужно включить больше уникальных, но редких слов (например, 1 или 2).\n","        max_df=0.008,            # Максимум N% документов (от 0 до 100%, включая доли процентов- 0.00001%)\n","        # Увеличьте значение (например, 0.9), чтобы включить больше слов, если они все еще являются значимыми для анализа.\n","        # Уменьшите (например, 0.008), если хотите исключить слишком распространенные слова (например, \"это\", \"завтра\").\n","        # ЗНАЧЕНИЕ ВЫШЕ НЕОБХОДИМО МЕНЯТЬ В ЗАВИСИМОСТИ ОТ ВЫВОДА.\n","        stop_words=russian_stopwords  # Убираем стоп-слова (либо russian_stopwords либо custom_stop_words)\n","    )\n","    tfidf_matrix = vectorizer.fit_transform(preprocessed_messages)\n","\n","    # Получаем слова и их веса\n","    feature_names = vectorizer.get_feature_names_out()\n","    tfidf_scores = tfidf_matrix.mean(axis=0).A1  # Средний вес для каждого слова\n","    keywords = list(zip(feature_names, tfidf_scores))\n","\n","    # Сортируем по важности\n","    keywords = sorted(keywords, key=lambda x: x[1], reverse=True)\n","\n","    return keywords[:top_n]\n","\n","# Основной код\n","def analyze_style(file_path, person_name, top_n=10):\n","    \"\"\"\n","    Анализирует стиль конкретного человека на основе JSON-файла с сообщениями.\n","\n","    Args:\n","        file_path: Путь к JSON-файлу.\n","        person_name: Имя человека, чьи сообщения нужно анализировать.\n","        top_n: Количество ключевых слов для извлечения.\n","    Returns:\n","        Список ключевых слов с их важностью.\n","    \"\"\"\n","    # Загружаем данные из JSON\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        data = json.load(file)\n","\n","    # Извлекаем сообщения конкретного человека\n","    messages = []\n","    for person in data.keys():\n","        for entry in data.get(person, []):\n","          if entry['from'] == person_name and len(entry['text']) <= 300:\n","              messages.append(entry['text'])\n","\n","    if not messages:\n","        print(f\"Нет сообщений для пользователя: {person_name}\")\n","        return []\n","\n","    # Анализируем стиль и возвращаем ключевые слова\n","    keywords = extract_style_keywords(messages, top_n=top_n)\n","    return keywords\n","\n","# Пример использования\n","file_path = '/content/drive/MyDrive/StyleMimic/messages.json'  # Укажите путь к JSON-файлу\n","person_name = 'Никитенко Николай'  # Имя человека, чьи сообщения нужно анализировать\n","\n","# Получаем ключевые слова\n","style_keywords = analyze_style(file_path, person_name, top_n=500)\n","print(style_keywords)\n","\n","# print(f\"Ключевые слова для {person_name}:\")\n","# for word, score in style_keywords:\n","#     print(f\"{word}: {score.round(4)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pFAAj1tFnWFi","executionInfo":{"status":"ok","timestamp":1734020626063,"user_tz":-300,"elapsed":3925,"user":{"displayName":"Николая Никитенко","userId":"05014441952842287654"}},"outputId":"75bd670a-154e-40de-d628-d6803855855a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["[('вообще', 0.007574188281805199), ('блин', 0.0075383892289878325), ('хотя', 0.007433132726025847), ('хочется', 0.007216718251804946), ('кстати', 0.007003198435915776), ('тобой', 0.006974369091176714), ('понимаю', 0.0068710090934666745), ('привет', 0.006731384314002649), ('милая', 0.006528497653681254), ('сделать', 0.006386087135997872), ('хочешь', 0.0063212866002707365), ('ааа', 0.005946017176986472), ('пока', 0.005666644328296685), ('делать', 0.005580723732083356), ('хотел', 0.005324731259254893), ('думаю', 0.0052694604695762105), ('сказал', 0.005268142455479998), ('очень сильно', 0.005159414706625526), ('сильно люблю', 0.0036808978909536184)]\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}